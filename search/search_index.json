{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"To use computer vision for video analysis, we have to train a model with annotated video frames. Next to the scripts in this repository, we use the scripts for creating and working with annotations from MIDA-scene-detection . In the future we may merge these repositories. This guide covers the following steps of the process: Training a model using labeled (i.e. annotated) frames; Evaluating a model on annotated frames; Making predictions for not yet annotated frames; Improving the annotations using ELAN, starting from predictions. Additionally, there are DIANNA installation instructions . DIANNA is a tool that tries to explain how a model classifies data.","title":"Overview"},{"location":"detect-with-yolo-model/","text":"Prepare data to run on \u00b6 Run YOLO model \u00b6 Command goes here. While it is possible to run YOLOv5 locally, it is highly advisable to use a machine with a GPU. See our instructions on running detection on the ALICE HPC to understand how we used ALICE. Filter false positives using a binary classifier \u00b6 Our YOLO model may predict a lot of false positives, even after increasing the number of training examples and epochs. We can try to remove false positives using a Keras-based binary classifier that works on the cropped images that YOLO thinks are symbols. Collect training data \u00b6 In our case only one symbol had many false positives. That means we only need to collect a sample of true and false positives for that symbol and we can train a binary classifier on this sample. Copy a set of correctly identified crops into one directory and a set of incorrectly identified crops into another. It helps if the directory names are such that the false positives come first, so that they internally get mapped to 0 , while true positives get 1 . When we then predict the correctness, the rounded confidence matches false and true positive, respectively. Train binary classifier \u00b6 See Train a model for instructions. Predict whether cropped images are the expected symbol \u00b6 Run the new classifier on each crop directory that YOLO made for the symbol. You could use predict_binary_images.py for this. cd yolo_model_5-recognitions for I in { 1 ..54 } ; do mkdir -p ${ I } /crops unzip -u ${ I } /ep ${ I } _211984_recognitions.zip '*.jpg' -d ${ I } /crops python ~/git/MIDA-video-analysis/predict_binary_images.py --model-directory ~/surfdrive/Projecten/MIDA/Mustafa/models/star_verifier --images-directory ${ I } /crops --pattern '*/*/star_of_david/*.jpg' --output ${ I } /star_results.csv done Remove incorrectly identified symbols \u00b6 We need to remove the lines from the predictions text files that correspond to images that our classifier thinks are not the symbol we are looking for. The yolo_zip_summary.py script provides the -a (or --aux-results ) option that expects a tuple of class label (as text) and corresponding results file. The threshold for including images based on the auxiliary classification is set to 0.3. This option can be repeated, so you could use auxiliary classification results for multiple symbols. cd yolo_model_5-recognitions for I in { 1 ..54 } ; do python ~/git/MIDA-video-analysis/yolo_zip_summary.py -a star_of_david ${ I } /star_results.csv --min-confidence 0 .4 --output-csv ${ I } /ep ${ I } -0_4-counts.csv --delete-labels pentagram,keys_of_heaven ${ I } /ep ${ I } _211984_recognitions.zip done Summarise results \u00b6 Run yolo summarise \u00b6 Run yolo_zip_summary.py for zipped results (such as the results from ALICE). cd yolo_model_5-recognitions for I in { 1 ..54 } ; do python ~/git/MIDA-video-analysis/yolo_zip_summary.py --min-confidence 0 .4 --output-csv ${ I } /ep ${ I } -0_4-counts.csv --delete-labels pentagram,keys_of_heaven ${ I } /ep ${ I } _211984_recognitions.zip done Combine with detected scenes \u00b6 Run match_symbols_to_scenes.py . cd yolo_model_5-recognitions/.. for I in { 1 ..54 } ; do python ~/git/MIDA-video-analysis/match_symbols_to_scenes.py \\ --symbols yolo_model_5-recognitions/ ${ I } /ep ${ I } -0_4-counts.csv \\ --scenes scene-detection-results/ ${ I } /scenes.csv \\ --output yolo_model_5-recognitions/ ${ I } /ep ${ I } -0_4-with-scenes.csv done Concatenate files per episode into one file \u00b6 We used csvstack . cd yolo_model_5-recognitions csvstack --filenames { 1 ..54 } /*0_4-with-scenes.csv > all_recognitions-0_4-with-scenes.csv If the CSV files are not similarly structured, you can concatenate them using the join_results.py script. cd yolo_model_5-recognitions python ~/git/MIDA-video-analysis/join_results.py -o all_recognitions-0_4-with-scenes.csv { 1 ..54 } /*0_4-with-scenes.csv Count symbols per scene \u00b6 Not implemented yet, but you could load the combined results into a spreadsheet application and create a pivot table that shows the number of symbols by episode and scene.","title":"Detect symbols with YOLO model"},{"location":"detect-with-yolo-model/#prepare-data-to-run-on","text":"","title":"Prepare data to run on"},{"location":"detect-with-yolo-model/#run-yolo-model","text":"Command goes here. While it is possible to run YOLOv5 locally, it is highly advisable to use a machine with a GPU. See our instructions on running detection on the ALICE HPC to understand how we used ALICE.","title":"Run YOLO model"},{"location":"detect-with-yolo-model/#filter-false-positives-using-a-binary-classifier","text":"Our YOLO model may predict a lot of false positives, even after increasing the number of training examples and epochs. We can try to remove false positives using a Keras-based binary classifier that works on the cropped images that YOLO thinks are symbols.","title":"Filter false positives using a binary classifier"},{"location":"detect-with-yolo-model/#collect-training-data","text":"In our case only one symbol had many false positives. That means we only need to collect a sample of true and false positives for that symbol and we can train a binary classifier on this sample. Copy a set of correctly identified crops into one directory and a set of incorrectly identified crops into another. It helps if the directory names are such that the false positives come first, so that they internally get mapped to 0 , while true positives get 1 . When we then predict the correctness, the rounded confidence matches false and true positive, respectively.","title":"Collect training data"},{"location":"detect-with-yolo-model/#train-binary-classifier","text":"See Train a model for instructions.","title":"Train binary classifier"},{"location":"detect-with-yolo-model/#predict-whether-cropped-images-are-the-expected-symbol","text":"Run the new classifier on each crop directory that YOLO made for the symbol. You could use predict_binary_images.py for this. cd yolo_model_5-recognitions for I in { 1 ..54 } ; do mkdir -p ${ I } /crops unzip -u ${ I } /ep ${ I } _211984_recognitions.zip '*.jpg' -d ${ I } /crops python ~/git/MIDA-video-analysis/predict_binary_images.py --model-directory ~/surfdrive/Projecten/MIDA/Mustafa/models/star_verifier --images-directory ${ I } /crops --pattern '*/*/star_of_david/*.jpg' --output ${ I } /star_results.csv done","title":"Predict whether cropped images are the expected symbol"},{"location":"detect-with-yolo-model/#remove-incorrectly-identified-symbols","text":"We need to remove the lines from the predictions text files that correspond to images that our classifier thinks are not the symbol we are looking for. The yolo_zip_summary.py script provides the -a (or --aux-results ) option that expects a tuple of class label (as text) and corresponding results file. The threshold for including images based on the auxiliary classification is set to 0.3. This option can be repeated, so you could use auxiliary classification results for multiple symbols. cd yolo_model_5-recognitions for I in { 1 ..54 } ; do python ~/git/MIDA-video-analysis/yolo_zip_summary.py -a star_of_david ${ I } /star_results.csv --min-confidence 0 .4 --output-csv ${ I } /ep ${ I } -0_4-counts.csv --delete-labels pentagram,keys_of_heaven ${ I } /ep ${ I } _211984_recognitions.zip done","title":"Remove incorrectly identified symbols"},{"location":"detect-with-yolo-model/#summarise-results","text":"","title":"Summarise results"},{"location":"detect-with-yolo-model/#run-yolo-summarise","text":"Run yolo_zip_summary.py for zipped results (such as the results from ALICE). cd yolo_model_5-recognitions for I in { 1 ..54 } ; do python ~/git/MIDA-video-analysis/yolo_zip_summary.py --min-confidence 0 .4 --output-csv ${ I } /ep ${ I } -0_4-counts.csv --delete-labels pentagram,keys_of_heaven ${ I } /ep ${ I } _211984_recognitions.zip done","title":"Run yolo summarise"},{"location":"detect-with-yolo-model/#combine-with-detected-scenes","text":"Run match_symbols_to_scenes.py . cd yolo_model_5-recognitions/.. for I in { 1 ..54 } ; do python ~/git/MIDA-video-analysis/match_symbols_to_scenes.py \\ --symbols yolo_model_5-recognitions/ ${ I } /ep ${ I } -0_4-counts.csv \\ --scenes scene-detection-results/ ${ I } /scenes.csv \\ --output yolo_model_5-recognitions/ ${ I } /ep ${ I } -0_4-with-scenes.csv done","title":"Combine with detected scenes"},{"location":"detect-with-yolo-model/#concatenate-files-per-episode-into-one-file","text":"We used csvstack . cd yolo_model_5-recognitions csvstack --filenames { 1 ..54 } /*0_4-with-scenes.csv > all_recognitions-0_4-with-scenes.csv If the CSV files are not similarly structured, you can concatenate them using the join_results.py script. cd yolo_model_5-recognitions python ~/git/MIDA-video-analysis/join_results.py -o all_recognitions-0_4-with-scenes.csv { 1 ..54 } /*0_4-with-scenes.csv","title":"Concatenate files per episode into one file"},{"location":"detect-with-yolo-model/#count-symbols-per-scene","text":"Not implemented yet, but you could load the combined results into a spreadsheet application and create a pivot table that shows the number of symbols by episode and scene.","title":"Count symbols per scene"},{"location":"evaluate-model/","text":"Use already labeled frames to see how many predictions are correct. To evaluate a model, run: $ python eval_multi_hot_model.py model_dir image_dir labels.csv results.csv In this command: model_dir is the directory that contains the model; image_dir is the directory that contains subdirectories with the images; labels.csv is the CSV file holding the filename and label(s) for each image, which should be structured the same as the file used for training; results.csv is a filename that the summary of predictions will be in.","title":"Evaluating a model"},{"location":"improve-annotations/","text":"After converting frame predictions to intervals, we can load them into ELAN. In ELAN, we delete or adjust the predicted intervals, or add new ones if symbols were missed. We can then create more accurate annotations and train a new model. Convert frame predictions to interval-based annotations \u00b6 Adjust intervals in ELAN \u00b6 Convert intervals to frame labels \u00b6","title":"Using predictions to improve annotations"},{"location":"improve-annotations/#convert-frame-predictions-to-interval-based-annotations","text":"","title":"Convert frame predictions to interval-based annotations"},{"location":"improve-annotations/#adjust-intervals-in-elan","text":"","title":"Adjust intervals in ELAN"},{"location":"improve-annotations/#convert-intervals-to-frame-labels","text":"","title":"Convert intervals to frame labels"},{"location":"install-dianna/","text":"Installing DIANNA , especially on an M1 Apple computer, requires some thought. Installing on an Apple M1 computer \u00b6 Because not all packages in PyPI have been built for the M1's ARM architecture yet, we start with the miniforge distribution of Conda. From here, these instructions may get outdated, as many packages are becoming available for ARM in PyPI. Some DIANNA dependencies, however, still need to be installed from conda-forge . These instructions have worked with MacOS 12 (Monterey), but may not work with MacOS 11, because Apple's M1-optimised version of Tensorflow is only supported on MacOS 12. Create and activate a conda environment for DIANNA: $ conda create -n dianna $ conda activate dianna Install Python 3.9 (per the DIANNA installation instructions ): $ conda install python = 3 .9 Install Tensorflow (per the Apple instructions ): $ conda install -c apple tensorflow-deps $ pip install tensorflow-macos $ pip install tensorflow-metal Install other dependencies: $ conda install onnxruntime onnx numba llvmlite $ pip install jupyter Comment out the tensorflow dependency from DIANNA's setup.cfg and then install DIANNA itself from the cloned git repository: $ cd path/to/dianna $ pip install -e .","title":"Installing DIANNA"},{"location":"install-dianna/#installing-on-an-apple-m1-computer","text":"Because not all packages in PyPI have been built for the M1's ARM architecture yet, we start with the miniforge distribution of Conda. From here, these instructions may get outdated, as many packages are becoming available for ARM in PyPI. Some DIANNA dependencies, however, still need to be installed from conda-forge . These instructions have worked with MacOS 12 (Monterey), but may not work with MacOS 11, because Apple's M1-optimised version of Tensorflow is only supported on MacOS 12. Create and activate a conda environment for DIANNA: $ conda create -n dianna $ conda activate dianna Install Python 3.9 (per the DIANNA installation instructions ): $ conda install python = 3 .9 Install Tensorflow (per the Apple instructions ): $ conda install -c apple tensorflow-deps $ pip install tensorflow-macos $ pip install tensorflow-metal Install other dependencies: $ conda install onnxruntime onnx numba llvmlite $ pip install jupyter Comment out the tensorflow dependency from DIANNA's setup.cfg and then install DIANNA itself from the cloned git repository: $ cd path/to/dianna $ pip install -e .","title":"Installing on an Apple M1 computer"},{"location":"make-predictions/","text":"When you have a model that is good enough, you can use it to predict whether symbols are visible on other unlabeled frames. Predict a single label with a binary classification model \u00b6 Use predict_binary_images.py . Make predictions from a directory of (directories of) images \u00b6 To make predictions using a model, run: $ python predict_images.py model_dir image_dir predictions.csv In this command: model_dir is the directory that contains the model; image_dir is the directory that contains subdirectories with the images; predictions.csv is the filename in which predictions will be saved. Make predictions for files in a CSV \u00b6 The other script that runs the model is based on the script for evaluation. It reads the filenames of the frames, as well as the expected labels from a CSV file. We therefore need to create the CSV file first. Create frame list \u00b6 This is done with add_empty_labels_to_frames.py . Make predictions from the frame list \u00b6 Use predict_images_from_frame.py . Look at the statistics \u00b6","title":"Make predictions"},{"location":"make-predictions/#predict-a-single-label-with-a-binary-classification-model","text":"Use predict_binary_images.py .","title":"Predict a single label with a binary classification model"},{"location":"make-predictions/#make-predictions-from-a-directory-of-directories-of-images","text":"To make predictions using a model, run: $ python predict_images.py model_dir image_dir predictions.csv In this command: model_dir is the directory that contains the model; image_dir is the directory that contains subdirectories with the images; predictions.csv is the filename in which predictions will be saved.","title":"Make predictions from a directory of (directories of) images"},{"location":"make-predictions/#make-predictions-for-files-in-a-csv","text":"The other script that runs the model is based on the script for evaluation. It reads the filenames of the frames, as well as the expected labels from a CSV file. We therefore need to create the CSV file first.","title":"Make predictions for files in a CSV"},{"location":"make-predictions/#create-frame-list","text":"This is done with add_empty_labels_to_frames.py .","title":"Create frame list"},{"location":"make-predictions/#make-predictions-from-the-frame-list","text":"Use predict_images_from_frame.py .","title":"Make predictions from the frame list"},{"location":"make-predictions/#look-at-the-statistics","text":"","title":"Look at the statistics"},{"location":"run-on-alice/","text":"Run scene detection for all episodes \u00b6 Run an array job. Training YOLOv5 with our data \u00b6 Detecting objects in all episodes \u00b6 Run an array job.","title":"Running YOLOv5 on the ALICE HPC"},{"location":"run-on-alice/#run-scene-detection-for-all-episodes","text":"Run an array job.","title":"Run scene detection for all episodes"},{"location":"run-on-alice/#training-yolov5-with-our-data","text":"","title":"Training YOLOv5 with our data"},{"location":"run-on-alice/#detecting-objects-in-all-episodes","text":"Run an array job.","title":"Detecting objects in all episodes"},{"location":"train-a-model/","text":"Overview \u00b6 We have worked on various Keras -based models to predict the visibility of symbols in frames from the TV series. As these models classify images into showing symbol and not showing symbol , we call them classification models . If there is only one symbol of interest, we could put the images with the symbol in one directory and the images without the symbol in another. This process is explained in Train a binary classifier . When we want a model to predict the appearance of multiple different symbols, potentially in the same frame, using directories to separate the classes becomes intractible. In that case we have to create a CSV file that contains the filenames of the images and for each symbol a column that indicates the visibility of the symbol for each image. This process is explained in Train a multi-label model . Train a binary classifier from image directories \u00b6 The script build_binary_classifier.py creates a binary classifier from images in subdirectories of a specific directory. Image directory setup \u00b6 Images must go into the directory corresponding to their class. For example, if your images belong either to class1 or class2 , you would use this directory structure: images_directory/ class1/ class2/ If you have lots of images, you can put them in subdirectories under class1 or class2 . Training the binary classifier \u00b6 The training script takes several required options, as explained in the command's --help : $ python build_binary_classifier.py --help Usage: build_binary_classifier.py [OPTIONS] Train a binary classifier for images. The location of the trained model is based on the --model-basedir and --model-name options. Options: -i, --image-dir PATH Directory with images for training and evaluation [required] --model-name TEXT Name of the model (must not include spaces) [required] --model-basedir PATH Base directory to store all models [required] --epochs INTEGER [default: 50] --seed INTEGER [default: 42] --split FLOAT [default: 0.2] --help Show this message and exit. The path to give to --image-dir in the previous example would be the path to image_directory , relative or absolute. Train a multi-label model from images and metadata \u00b6 Create training data \u00b6 To train a model, we need images with labels, and a name for the model. We expect images to be in subdirectories of a given directory. The filenames and corresponding labels should be in a CSV file. Filenames must be in a column filename and must be relative to the given directory. Names of columns with (numeric 0 or 1) labels must end in _visible . Minimal example of a labels file: filename pentagram_visible star_visible image001.jpg 1 0 image002.jpg 1 1 image003.jpg 0 1 image004.jpg 0 0 A model trained on these images and labels will have two outputs: pentagram and star . Run the training \u00b6 To train a model, run: $ python build_multi_hot_model.py image_dir labels.csv output_model_dir In this command: image_dir is the directory that contains subdirectories with the images; labels.csv is the CSV file holding the filename and label(s) for each image; output_model_dir is the name of the model and the directory that it will be saved to.","title":"Training a classification model"},{"location":"train-a-model/#overview","text":"We have worked on various Keras -based models to predict the visibility of symbols in frames from the TV series. As these models classify images into showing symbol and not showing symbol , we call them classification models . If there is only one symbol of interest, we could put the images with the symbol in one directory and the images without the symbol in another. This process is explained in Train a binary classifier . When we want a model to predict the appearance of multiple different symbols, potentially in the same frame, using directories to separate the classes becomes intractible. In that case we have to create a CSV file that contains the filenames of the images and for each symbol a column that indicates the visibility of the symbol for each image. This process is explained in Train a multi-label model .","title":"Overview"},{"location":"train-a-model/#train-a-binary-classifier-from-image-directories","text":"The script build_binary_classifier.py creates a binary classifier from images in subdirectories of a specific directory.","title":"Train a binary classifier from image directories"},{"location":"train-a-model/#image-directory-setup","text":"Images must go into the directory corresponding to their class. For example, if your images belong either to class1 or class2 , you would use this directory structure: images_directory/ class1/ class2/ If you have lots of images, you can put them in subdirectories under class1 or class2 .","title":"Image directory setup"},{"location":"train-a-model/#training-the-binary-classifier","text":"The training script takes several required options, as explained in the command's --help : $ python build_binary_classifier.py --help Usage: build_binary_classifier.py [OPTIONS] Train a binary classifier for images. The location of the trained model is based on the --model-basedir and --model-name options. Options: -i, --image-dir PATH Directory with images for training and evaluation [required] --model-name TEXT Name of the model (must not include spaces) [required] --model-basedir PATH Base directory to store all models [required] --epochs INTEGER [default: 50] --seed INTEGER [default: 42] --split FLOAT [default: 0.2] --help Show this message and exit. The path to give to --image-dir in the previous example would be the path to image_directory , relative or absolute.","title":"Training the binary classifier"},{"location":"train-a-model/#train-a-multi-label-model-from-images-and-metadata","text":"","title":"Train a multi-label model from images and metadata"},{"location":"train-a-model/#create-training-data","text":"To train a model, we need images with labels, and a name for the model. We expect images to be in subdirectories of a given directory. The filenames and corresponding labels should be in a CSV file. Filenames must be in a column filename and must be relative to the given directory. Names of columns with (numeric 0 or 1) labels must end in _visible . Minimal example of a labels file: filename pentagram_visible star_visible image001.jpg 1 0 image002.jpg 1 1 image003.jpg 0 1 image004.jpg 0 0 A model trained on these images and labels will have two outputs: pentagram and star .","title":"Create training data"},{"location":"train-a-model/#run-the-training","text":"To train a model, run: $ python build_multi_hot_model.py image_dir labels.csv output_model_dir In this command: image_dir is the directory that contains subdirectories with the images; labels.csv is the CSV file holding the filename and label(s) for each image; output_model_dir is the name of the model and the directory that it will be saved to.","title":"Run the training"},{"location":"train-a-yolo-model/","text":"Collect training data \u00b6 Training data consist of images and rectangular annotations of objects. The object labels are in a text file that is linked to the image through the file name. Multiple objects in a single image go on separate lines. It is advisable to use a tool like https://makesense.ai/ . Warning If you create various datasets, make sure to use the same labels in each of them. YOLO expects numbers as labels and training will not give expected results if in one dataset the symbol A maps to 2 and in another dataset it maps to 0. Various tips are available in the YOLOv5 custom data guide . Prepare training configuration \u00b6 The YOLOv5 custom data guide has the most important information. If you want to adjust training parameters like random rotation, you have to provide your own hyperparameters in a YAML file. Run the training \u00b6 Again, see the YOLOv5 custom data guide for the current instructions. See our instructions on running the training on the ALICE HPC to understand how we can speed up the process.","title":"Train a YOLOv5 model"},{"location":"train-a-yolo-model/#collect-training-data","text":"Training data consist of images and rectangular annotations of objects. The object labels are in a text file that is linked to the image through the file name. Multiple objects in a single image go on separate lines. It is advisable to use a tool like https://makesense.ai/ . Warning If you create various datasets, make sure to use the same labels in each of them. YOLO expects numbers as labels and training will not give expected results if in one dataset the symbol A maps to 2 and in another dataset it maps to 0. Various tips are available in the YOLOv5 custom data guide .","title":"Collect training data"},{"location":"train-a-yolo-model/#prepare-training-configuration","text":"The YOLOv5 custom data guide has the most important information. If you want to adjust training parameters like random rotation, you have to provide your own hyperparameters in a YAML file.","title":"Prepare training configuration"},{"location":"train-a-yolo-model/#run-the-training","text":"Again, see the YOLOv5 custom data guide for the current instructions. See our instructions on running the training on the ALICE HPC to understand how we can speed up the process.","title":"Run the training"}]}